{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy vs Nltk\n",
    "\n",
    "![imageSpacyVsNltk](../images/spacyVsnltk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to use the package we will use below we need to run this code in your terminal \n",
    "```bash \n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.strange love pizza .\n",
      "Dr.strange\n",
      "love\n",
      "pizza\n",
      ".\n",
      "Iron man love beef\n",
      "Iron\n",
      "man\n",
      "love\n",
      "beef\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")## this line load specific package \n",
    "\n",
    "\n",
    "\n",
    "## now let's do some sentence  and word tokenization tokenization in spacy \n",
    "\n",
    "\n",
    "doc = nlp(\"Dr.strange love pizza . Iron man love beef\")\n",
    "\n",
    "\n",
    "for sentence in doc.sents: \n",
    "  print(sentence)## untile here is sentence tokenization \n",
    "  for word in sentence : \n",
    "    print (word)## here word token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nltk \n",
    "\n",
    " below particualr tokenizers requires the PUnkst sentence tokenization models to be installed look [here](https://www.nltk.org/api/nltk.tokenize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ubey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```py nltk.download()``` \n",
    " this line will open nltk downloader because when you install nltk you install the base library but it doesn't intall  all the packages related to  different languages and different tokenization schemes , and there is models inside nltk downloader that you can install   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Dr.string loves pizza.', 'Iron man loves beef']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = ''' Dr.string loves pizza. Iron man loves beef '''\n",
    "sent_tokenize(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.string', 'loves', 'pizza', '.', 'Iron', 'man', 'loves', 'beef']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
    "... two of them.\\n\\nThanks.'''\n",
    "word_tokenize(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
